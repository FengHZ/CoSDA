---
DataConfig:
  dataset: "VisDA2017"

ModelConfig:
  backbone: "vit_base_patch16_224"
  channels_per_group: 0
  pretrain_strategy: "shot"
  bottleneck_dim: 256

TrainingConfig:
  batch_size: 64
  # The total data numbers we use in each epoch
  epoch_samples: 1024
  total_epochs: 60
  # optimizer
  optimizer: "SGD"
  momentum: 0.9
  # regularization
  weight_decay: 0.0005
  # We decay learning rate from begin value to end value with cosine annealing schedule
  learning_rate_begin: 0.002
  learning_rate_end: 0.0001
  # We use EMA to retain the accuracy in source domain
  ema: False

DAConfig:
  source_domain: "Synthetic"
  target_domain: "Real"
  method: "AaD"

AaDConfig: # batch_size: 64 is proper
  k: 8
  alpha: 0.4 # the param lambda in the paper.
  gamma: 0.96
  ema: False

DataAugConfig:
  method: "identity" # method can be "identity", "edgemix"
---
DataConfig:
  dataset: "DomainNet_mini"

ModelConfig:
  backbone: "resnet50"
  channels_per_group: 0
  pretrain_strategy: "shot"
  pretrain_beta: 0.1
  bottleneck_dim: 512

TrainingConfig:
  batch_size: 256
  # The total data numbers we use in each epoch
  epoch_samples: 30000
  total_epochs: 40
  # optimizer
  optimizer: "SGD"
  momentum: 0.9
  # regularization
  weight_decay: 0.002
  # We decay learning rate from begin value to end value with cosine annealing schedule
  learning_rate_begin: 0.001
  learning_rate_end: 0.0002
  # We use EMA to retain the accuracy in source domain
  ema: False

DAConfig:
  source_domain: "infograph"
  target_domain: "real"
  method: "AaD"

AaDConfig: # batch_size: 64 is proper
  k: 4
  alpha: 0.4 # the param lambda in the paper.
  gamma: 0.96
  ema: False

DataAugConfig:
  method: "edgemix" # method can be "identity", "edgemix"
  lam: 0.9 # edge_mixup_image = lam * origin + (1 - lam) * edge
  finetune_epochs: 2 # use last {} epochs to finetune
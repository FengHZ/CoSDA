---
DataConfig:
  dataset: "DomainNet"

ModelConfig:
  backbone: "vit_base_patch16_224.augreg2_in21k_ft_in1k"
  channels_per_group: 0
  pretrain_strategy: "shot"
  bottleneck_dim: 512

TrainingConfig:
  batch_size: 256
  # The total data numbers we use in each epoch
  epoch_samples: 30000
  total_epochs: 40
  # optimizer
  optimizer: "SGD"
  momentum: 0.9
  # regularization
  weight_decay: 0.0005
  # We decay learning rate from begin value to end value with cosine annealing schedule
  learning_rate_begin: 0.002
  learning_rate_end: 0.001
  ema: True
  tao_begin: 0.95
  tao_end: 0.99

DAConfig:
  source_domain: "painting"
  target_domain: "sketch"
  method: "CoSDA"   

DistillConfig:
  beta: 2
  reg_alpha: 0.05
  temperature_begin: 0.07
  temperature_end: 0.07
  # As stated in paper, we gradually increase confidence_gate from low to high
  confidence_gate_begin: 0.4
  confidence_gate_end: 0.7

DataAugConfig:
  method: "identity" # method can be "identity", "edgemix"